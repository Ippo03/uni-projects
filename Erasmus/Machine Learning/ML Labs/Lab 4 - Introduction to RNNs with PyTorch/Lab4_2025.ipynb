{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eakOC9L9Smj1"
      },
      "source": [
        "# Lab 4 – Introduction to RNNs with PyTorch\n",
        "\n",
        "---\n",
        "\n",
        "How to Use This Notebook\n",
        "---\n",
        "\n",
        "**Recommended Setup**\n",
        "- For the best experience, **run this notebook on [Google Colab](https://colab.research.google.com/)**—especially if your local machine is slow.  \n",
        "- In Colab, **enable GPU support** by going to:  \n",
        "  `Runtime > Change runtime type > Hardware accelerator > GPU`\n",
        "\n",
        "\n",
        "**Homework Tasks**\n",
        "\n",
        " - Homework tasks are clearly marked throughout the notebook in the following format:\n",
        "\n",
        "   > ---\n",
        "\n",
        "   > <span style=\"color:red\"><b>TASK X</b> - [<i>some text</i>]:</span>\n",
        "\n",
        "   > ---\n",
        "\n",
        "   > ```Your code ....```\n",
        "\n",
        "   > ---\n",
        "\n",
        "   > *End of Task X.* [*Instructions for passing*]\n",
        "\n",
        " - For each task:\n",
        "   - **Complete the code** where indicated.\n",
        "   - **Upload the required results** from each task to **Homework 4 – Code** on [NextIlearn](https://nextilearn.dsv.su.se).\n",
        "\n",
        " - Once you've finished all the tasks:\n",
        "   Submit your **entire completed notebook (including your code!)** to **Homework 4 – Notebook** on [NextIlearn](https://nextilearn.dsv.su.se).\n",
        "\n",
        "**Important:**  \n",
        "Your submission will **only be graded if both files** (code + notebook) are uploaded **before the deadline**. Late submissions are **not accepted**, regardless of technical issues like bad internet connection.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this lab, we will explore **Recurrent Neural Networks (RNNs)** using **PyTorch**. This lab is hands-on and designed to be run during class time. Each step is followed by detailed explanations and code examples.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Objectives\n",
        "- Understand RNN architecture and its components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEfK7dtmTZEI"
      },
      "source": [
        "In the previous lab we learned about CNNs, now we move on to RNNs! Creating a RNN network is much simpler than creating CNN networks which require many different convolutional filters and kernels. You can create a working RNN network only with few lines. Here we will first look into different models on a simple synthetic data. After that we will investigate more by creating a model for sentimental analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx54_rItVaAe"
      },
      "source": [
        "### Introduction and Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZF_r7VNNJj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from torchsummary import summary\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p34a7chxX4-g"
      },
      "source": [
        "We move models and data to ```device``` (cuda if available) for GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Ii8z6zNKCc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Setup device - use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-X-6hCcYD7A"
      },
      "source": [
        "Here we generate 10,000 time series instances of length 51 and use 70% as a training set. We will put first 50 data points to the model to predict the last data point. Each sample is a noisy combination of sine waves + random noise.\n",
        "\n",
        "Model goal is to predict the last time step given previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LliU5bErNM9c"
      },
      "outputs": [],
      "source": [
        "# Function to generate synthetic time series data\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))   # Wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))  # + Wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)    # + Noise\n",
        "    return series[..., np.newaxis].astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuB5Bv9hYdFZ"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "# Generate dataset\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "# Split into training and testing\n",
        "X_train = series[:7000, :n_steps]\n",
        "y_train = series[:7000, -1]\n",
        "X_test = series[7000:, :n_steps]\n",
        "y_test = series[7000:, -1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmRUiRpmYl4t"
      },
      "source": [
        "*You* need to check the special axis at the end to indicate the sequential data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQNLb8aZNPL2"
      },
      "outputs": [],
      "source": [
        "# Check shape\n",
        "print(\"Training X shape:\", X_train.shape)\n",
        "print(\"Training y shape:\", y_train.shape)\n",
        "print(\"Test X shape:\", X_test.shape)\n",
        "print(\"Test y shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf_wcc8JZD87"
      },
      "source": [
        "Here you can check few examples of our synthetic time series. Our objective is to create a model to predict these blue Xs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHFbt77oNRo0"
      },
      "outputs": [],
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\"):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bx\", markersize=10)\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"ro\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_test[col, :, 0], y_test[col, 0], y_label=(\"$x(t)$\" if col==0 else None))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OM3K6QBcXrP"
      },
      "source": [
        "We will create the following models on the same dataset:\n",
        " - Fully connected network\n",
        " - Simple RNN\n",
        " - Deep RNN\n",
        " - LSTM\n",
        " - GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aL6lPKUcxpP"
      },
      "source": [
        "### Fully connected network\n",
        "\n",
        "Fully connected network can also handle sequential data by regarding each time point input independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyfQsX98OK1B"
      },
      "outputs": [],
      "source": [
        "# Fully Connected Network (FCN) for time series forecasting\n",
        "\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, n_steps):\n",
        "        super(FCN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(n_steps, 1)  # Maps all time steps to a single output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model_fcn = FCN(n_steps).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCUj373ACm87"
      },
      "source": [
        "Why Flatten?\n",
        "- Sequential inputs are treated as independent features — no memory of order.\n",
        "- This makes FCNs fundamentally different from RNNs that remember sequence!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-2puzHROYuH"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_fcn.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDtHWZ7_Cm87"
      },
      "source": [
        "Loss:\n",
        "- We use Mean Squared Error (MSE) because this is a regression task.\n",
        "\n",
        "Optim:\n",
        "- Adam optimizer is generally a good default for deep learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRddEvIeNU4C"
      },
      "outputs": [],
      "source": [
        "# Move data to device\n",
        "X_train_tensor = torch.tensor(X_train, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "\n",
        "# Training the model\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model_fcn.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_fcn(X_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdoWPjuFCm87"
      },
      "source": [
        "Prediction:\n",
        "- After .eval(), we disable gradients (torch.no_grad()) for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKfcB54tNX3b"
      },
      "outputs": [],
      "source": [
        "# Move test data to device\n",
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "# Evaluate\n",
        "model_fcn.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = model_fcn(X_test_tensor)\n",
        "\n",
        "# Move back to CPU for plotting\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "\n",
        "# Plot first test sample prediction\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_test_pred[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZmU0f-etHA"
      },
      "source": [
        "### Simple RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Az8em_HOzcN"
      },
      "outputs": [],
      "source": [
        "# Basic RNN model\n",
        "\n",
        "class SimpleRNNModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=1):\n",
        "        super(SimpleRNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Final output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)  # Ignore hidden state\n",
        "        out = out[:, -1, :]   # Take the output at the last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model_rnn = SimpleRNNModel(input_size=1, hidden_size=1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCHKcdgDNakW"
      },
      "outputs": [],
      "source": [
        "summary(model_rnn, (50, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuBv2HIdO6BW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_rnn.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De2EKlICNfyG"
      },
      "outputs": [],
      "source": [
        "# Reshape X to (batch_size, time_steps, features)\n",
        "X_train_tensor = torch.tensor(X_train, device=device)  # already loaded earlier\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "\n",
        "n_epochs = 30\n",
        "for epoch in range(n_epochs):\n",
        "    model_rnn.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_rnn(X_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QikzBTHhNlIw"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "model_rnn.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = model_rnn(X_test_tensor)\n",
        "\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "\n",
        "# Plot prediction\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_test_pred[0, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSGjbQ0lgYVD"
      },
      "source": [
        "### Deep RNN\n",
        "We can have more than one RNN layers to let the network learn a bit more complex patterns.\n",
        "\n",
        "For Deep RNN Layers:\n",
        "\n",
        "num_layers= n tells PyTorch to stack n RNN layers internally.\n",
        "\n",
        "PyTorch automatically handles passing hidden states between layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR2xnZhYPXGz"
      },
      "outputs": [],
      "source": [
        "# Deep RNN model with multiple RNN layers\n",
        "\n",
        "class DeepRNNModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, num_layers=3):\n",
        "        super(DeepRNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Predict one output\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]   # Take the last output\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model_deep_rnn = DeepRNNModel(input_size=1, hidden_size=20, num_layers=3).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owMYnHIXNpYe"
      },
      "outputs": [],
      "source": [
        "summary(model_deep_rnn, (50, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzrf9twXPbC5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_deep_rnn.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tFthBydNsoZ"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model_deep_rnn.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_deep_rnn(X_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqXe0r7LNvBe"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "model_deep_rnn.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = model_deep_rnn(X_test_tensor)\n",
        "\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "\n",
        "# Plot prediction\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_test_pred[0, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcF32Gt3hvy8"
      },
      "source": [
        "### LSTM & GRU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3JMh3sDCm88"
      },
      "source": [
        "How LSTM differs from SimpleRNN:\n",
        "\n",
        "- LSTM has gates (input gate, forget gate, output gate).\n",
        "\n",
        "- These gates help the model decide what to remember or forget.\n",
        "\n",
        "- Better for learning long-term dependencies.\n",
        "\n",
        "- Many deep LSTM models use dropout between layers (we’re keeping it simple here).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsFsFsriP5zm"
      },
      "outputs": [],
      "source": [
        "# LSTM model for time series forecasting\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, num_layers=2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]   # Take the last output\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model_lstm = LSTMModel(input_size=1, hidden_size=20, num_layers=2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsk4W_AiP7fJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKGI69uWNxhM"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model_lstm.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_lstm(X_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGRNfN2gNzzV"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "model_lstm.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = model_lstm(X_test_tensor)\n",
        "\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "\n",
        "# Plot prediction\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_test_pred[0, 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39P14PepQ-XV"
      },
      "outputs": [],
      "source": [
        "# GRU model for time series forecasting\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, num_layers=2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)  # Final prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]   # Use last time step output\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model_gru = GRUModel(input_size=1, hidden_size=20, num_layers=2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqJoeRBBRBUa"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_gru.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keOZTIlYN2i9"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, device=device)\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model_gru.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_gru(X_train_tensor)\n",
        "    loss = criterion(y_pred, y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5QmVYEsN5I8"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "model_gru.eval()\n",
        "with torch.no_grad():\n",
        "    y_test_pred = model_gru(X_test_tensor)\n",
        "\n",
        "y_test_pred = y_test_pred.cpu().numpy()\n",
        "\n",
        "# Plot prediction\n",
        "plot_series(X_test[0, :, 0], y_test[0, 0], y_test_pred[0, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baNrF2muh_ix"
      },
      "source": [
        "### More than one point?\n",
        "\n",
        "You can always let model predict more than one point. There can be many ways as follows:\n",
        "- Let model predict one step ahead and feed it again to the model to predict the next one.\n",
        "- **Let model predict ten steps all together at the last step.**\n",
        "- Let model predict ten steps all together at each time step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTvAD01hN7mn"
      },
      "outputs": [],
      "source": [
        "# Regenerate series for multi-step forecasting (predict 10 steps)\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "\n",
        "# Split into training, validation, and testing sets\n",
        "X_train = series[:7000, :n_steps]\n",
        "Y_train = series[:7000, -10:, 0]   # 10 future steps\n",
        "X_valid = series[7000:9000, :n_steps]\n",
        "Y_valid = series[7000:9000, -10:, 0]\n",
        "X_test = series[9000:, :n_steps]\n",
        "Y_test = series[9000:, -10:, 0]\n",
        "\n",
        "# Print shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"X_valid shape:\", X_valid.shape)\n",
        "print(\"Y_valid shape:\", Y_valid.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Y_test shape:\", Y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvHhoqXaCm9I"
      },
      "source": [
        "Why predict multiple points at once?\n",
        "- In real-world forecasting (e.g., weather, finance), predicting just the next point isn’t enough — we need future trajectories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67KfGNTCm9I"
      },
      "source": [
        "Note: In the previous parts of this lab we learned a built-in deep RNN, automatically stacking 3 layers internally. In the structure below you can see the manual stacking of two separate RNN layers.\n",
        "\n",
        " ```self.rnn1 = nn.RNN(...)```\n",
        "\n",
        "\n",
        "```self.rnn2 = nn.RNN(...)```\n",
        "\n",
        "you can use this structure when:\n",
        "\n",
        "- You need to predict multiple outputs (like forecasting next 10 time steps).\n",
        "\n",
        "- You want control between layers (e.g., add Dropout, BatchNorm, or attention).\n",
        "\n",
        "- You want to explore non-uniform RNN configurations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOduo9o1RhPo"
      },
      "outputs": [],
      "source": [
        "# Simple deep RNN model predicting multiple steps (10 outputs)\n",
        "\n",
        "class MultiStepRNNModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, output_size=10):\n",
        "        super(MultiStepRNNModel, self).__init__()\n",
        "        self.rnn1 = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn1(x)\n",
        "        out, _ = self.rnn2(out)\n",
        "        out = out[:, -1, :]   # Last time step output\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model_multistep = MultiStepRNNModel().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St7Ty7hKRlVW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_multistep.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Gwm0S-N_Xv"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, device=device)\n",
        "Y_train_tensor = torch.tensor(Y_train, device=device)\n",
        "\n",
        "\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    model_multistep.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model_multistep(X_train_tensor)\n",
        "    loss = criterion(y_pred, Y_train_tensor)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model_multistep.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model_multistep(X_valid_tensor)\n",
        "        val_loss = criterion(val_pred, Y_valid_tensor)\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEXssfeBCm9I"
      },
      "source": [
        "Plotting:\n",
        "- We visualize both actual future values and predicted ones over 10 time steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chL6OPcCOL5n"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "X_test_tensor = torch.tensor(X_test, device=device)\n",
        "\n",
        "model_multistep.eval()\n",
        "with torch.no_grad():\n",
        "    Y_test_pred = model_multistep(X_test_tensor)\n",
        "\n",
        "Y_test_pred = Y_test_pred.cpu().numpy()\n",
        "\n",
        "# Helper function to plot forecast\n",
        "def plot_multiple_forecasts(X, Y, Y_pred, n_steps=50, ahead=10):\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0], \"bo-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0], \"rx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "# Plot first sample\n",
        "plot_multiple_forecasts(X_test, Y_test, Y_test_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mxzbfZtCm9I"
      },
      "source": [
        "## Time Series Forecasting: Stock Price Prediction\n",
        "The task at hand is a classic example of time series forecasting, where the goal is to predict future values based on previously observed values. Specifically, we aim to predict the future stock prices of Apple Inc. (AAPL) using historical stock price data.\n",
        "\n",
        "The primary goal is to develop a predictive model that can accurately forecast future stock prices. By analyzing patterns and trends in historical data, the model will attempt to predict the closing prices of the stock for the upcoming days.\n",
        "\n",
        "## Challenges in Time Series Forecasting\n",
        "- **Temporal Dependency**: Unlike traditional regression tasks, time series data points are dependent on each other over time. Capturing this temporal dependency is crucial for accurate predictions.\n",
        "- **Trend and Seasonality**: Stock prices often exhibit trends (long-term increase or decrease) and seasonality (repeating patterns over specific periods). Identifying and modeling these components is essential.\n",
        "- **Noise and Volatility**: Financial data is typically noisy and can be highly volatile due to market conditions, economic events, and other external factors.\n",
        "- **Data Stationarity**: Many statistical models assume that the time series data is stationary, meaning its statistical properties do not change over time. Non-stationary data needs to be transformed for effective modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQtrcCUcCm9I"
      },
      "source": [
        "## Installing yfinance\n",
        "This cell installs the yfinance library, which is used to download historical market data from Yahoo Finance. This library will be used later to fetch stock prices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV2ZfDk0Cm9J",
        "outputId": "313dfb27-b1eb-4b44-f7df-8b80a733ea64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /opt/anaconda3/lib/python3.12/site-packages (0.2.56)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2024.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8LWO3fQCm9J"
      },
      "source": [
        "## Loading and Preparing the Dataset\n",
        "When we run the first line, the library fetches the historical stock data for Apple Inc. (AAPL) from Yahoo Finance for the date range specified, which is from January 1, 2010, to January 1, 2021. This data includes various columns such as 'Open', 'High', 'Low', 'Close', 'Volume', and 'Adj Close'. The data is returned as a Pandas DataFrame. The closing prices are extracted from the DataFrame and converted to a NumPy array of floats for numerical computations. Next, the data is normalized to the range [-1, 1] using MinMaxScaler, which helps in improving the performance and convergence rate of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Pk8QNXQQW6"
      },
      "outputs": [],
      "source": [
        "# Download historical stock data for Apple (AAPL)\n",
        "df = yf.download('AAPL', start='2010-01-01', end='2021-01-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xrzx30jCm9J"
      },
      "outputs": [],
      "source": [
        "# Focus only on closing prices\n",
        "data = df['Close'].values.astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMIQGbBbCm9J"
      },
      "outputs": [],
      "source": [
        "# Define sequence length for time series sequences\n",
        "seq_length = 30\n",
        "\n",
        "# Split into training and test sets (80/20)\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGFWx4dICm9J"
      },
      "source": [
        "Beware of data leakage when splitting/normalizing time series!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVXTW9ehCm9J"
      },
      "outputs": [],
      "source": [
        "# Normalize data separately to avoid data leakage\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1))\n",
        "test_data_normalized = scaler.transform(test_data.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDiP-r0qCm9J"
      },
      "source": [
        "Important Note:\n",
        "\n",
        "- Financial forecasting is often non-stationary and stock prices have heavy tails.\n",
        "\n",
        "- Stock prices are extremely hard to predict and models usually predict returns, not prices, in real finance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2N2IS8Cm9J"
      },
      "source": [
        "## Creating Sequences for Training\n",
        "The function create_sequences is defined to generate sequences of length 30 from the normalized data, where each sequence is used to predict the next value. The data is then split into training and testing sets, with 80% used for training and 20% for testing. Finally, the training and testing data are converted to PyTorch tensors, making them ready for model training and evaluation. This comprehensive preprocessing ensures that the data is appropriately formatted and scaled for the GRU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HtbXf-aCm9J"
      },
      "outputs": [],
      "source": [
        "# Function to create sequences and targets from data\n",
        "def create_sequences(data, seq_length):\n",
        "  # Initialize a list to store input sequences\n",
        "  xs = []\n",
        "  # Initialize a list to store target values\n",
        "  ys = []\n",
        "  for i in range(len(data) - seq_length):\n",
        "    # Extract a sequence of length seq_length\n",
        "    x = data[i:i+seq_length]\n",
        "    # Extract the next data point\n",
        "    y = data[i+seq_length]\n",
        "    # Append the sequence to the input list\n",
        "    xs.append(x)\n",
        "    # Append the target value to the target list\n",
        "    ys.append(y)\n",
        "  return np.array(xs), np.array(ys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uECPt_mCm9J"
      },
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "X_train, y_train = create_sequences(train_data_normalized, seq_length)\n",
        "X_test, y_test = create_sequences(test_data_normalized, seq_length)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub97by9VCm9J"
      },
      "source": [
        "### Create DataLoaders for Efficient Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfq8BtlQCm9J"
      },
      "outputs": [],
      "source": [
        "# Create datasets and loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQKgnkkbCm9J"
      },
      "source": [
        "## Defining the GRU Model\n",
        "In this cell, we define the architecture of the GRU (Gated Recurrent Unit) model using PyTorch's nn.Module. The GRUModel class is created, and the __init__ method initializes the parameters of the model, including input_size, hidden_size, output_size, and num_layers. The GRU layer is defined with the specified input size, hidden size, and number of layers. Additionally, a fully connected layer (nn.Linear) is added to map the GRU outputs to the desired output size. The forward method defines the forward pass of the model, where an initial hidden state is created, the input data is passed through the GRU layer, and the output of the last time step is passed through the fully connected layer to generate the final prediction. This model architecture is designed to capture temporal dependencies in the stock price data, making it well-suited for time series forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AcEww8cCm9J"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_prob=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        # Define the GRU layer\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "        # Define the fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        if h0 is None:\n",
        "            h0 = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size, device=x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Only use the last time step output\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu6DfjGdCm9J"
      },
      "source": [
        "## Initializing the Model, Loss Function, and Optimizer\n",
        "In this cell, we initialize the GRU model with specific hyperparameters. The input_size is set to 1, as we are using a single feature (the closing price). The hidden_size is set to 100, indicating the number of features in the hidden state. The output_size is set to 1, as we are predicting a single value (the next closing price). The num_layers is set to 2, indicating that the GRU will have two stacked layers. Dropout rate is set to 0.2 to prevent the model from overfitting.\n",
        "\n",
        "The mean squared error (MSELoss) is chosen as the loss function, which measures the average squared difference between the predicted and actual values. The Adam optimizer is initialized with a learning rate of 0.001, which will be used to update the model parameters during training. This setup prepares the model for the training process, defining how it will learn from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X1j52b1Cm9J"
      },
      "outputs": [],
      "source": [
        "# Initialize model with hyperparameters\n",
        "model = GRUModel(input_size=1, hidden_size=100, output_size=1, num_layers=2, dropout_prob=0.2)\n",
        "\n",
        "# Define Mean Squared Error loss\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oGrsyv4Cm9J"
      },
      "source": [
        "## Training the model\n",
        "This cell handles the training process of the GRU model. The model is trained for a specified number of epochs (num_epochs set to 50). For each epoch, the model is set to training mode using model.train(). The training data is loaded in batches using train_loader. For each batch, the optimizer gradients are zeroed, the model makes predictions, the loss is computed, and the loss is backpropagated. The optimizer then updates the model parameters. After each epoch, the model is set to evaluation mode using model.eval(), and the validation loss is computed using val_loader. This validation step ensures that the model's performance is monitored on unseen data, helping to prevent overfitting. The average validation loss is printed at the end of each epoch, providing insight into the model's progress and effectiveness.\n",
        "\n",
        "Then, The model is set to evaluation mode using model.eval() to ensure that it does not update its parameters during testing. The evaluation is done within a torch.no_grad() context to prevent gradient computation, making the process faster and more memory-efficient. The test data is loaded in batches using test_loader. For each batch, the model makes predictions, and the loss is computed using the mean squared error criterion. The cumulative test loss is computed by summing the loss for each batch. Finally, the average test loss is printed, providing an unbiased estimate of the model's performance on unseen data. This step is crucial to verify that the model generalizes well and is ready for real-world predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxNMiSpkQW3C"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs\n",
        "epochs = 50\n",
        "\n",
        "# For tracking losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for seqs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seqs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for seqs, labels in test_loader:\n",
        "            outputs = model(seqs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    # Record losses\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    test_losses.append(test_loss / len(test_loader))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch:3}: Train Loss = {running_loss / len(train_loader):.6f}, Test Loss = {test_loss / len(test_loader):.6f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGQarzduCm9K"
      },
      "source": [
        "## Making Predictions\n",
        "In this cell, we demonstrate how to use the trained GRU model to make predictions on new data. The model is set to evaluation mode using model.eval(), ensuring that no gradients are computed, and the model's parameters remain unchanged. Within a torch.no_grad() context, a sample input data is converted to a PyTorch tensor. The model then makes predictions based on this input data. The predicted values are printed, showcasing the model's ability to forecast future stock prices. This step illustrates the practical application of the trained model, allowing us to make informed predictions based on historical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHVO9It5Cm9K"
      },
      "outputs": [],
      "source": [
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to generate predictions\n",
        "def predict(model, loader):\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for seqs, _ in loader:\n",
        "            outputs = model(seqs)\n",
        "            preds.extend(outputs.squeeze().cpu().numpy())\n",
        "    return np.array(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOhqb1FUCm9K"
      },
      "outputs": [],
      "source": [
        "# Create a train eval loader without shuffle\n",
        "train_eval_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Generate predictions\n",
        "train_predictions = predict(model, train_eval_loader)\n",
        "test_predictions = predict(model, test_loader)\n",
        "\n",
        "# Inverse transform predictions and true values\n",
        "train_predictions = scaler.inverse_transform(train_predictions.reshape(-1, 1))\n",
        "test_predictions = scaler.inverse_transform(test_predictions.reshape(-1, 1))\n",
        "y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP3AQhhYCm9K"
      },
      "source": [
        "### Plot Predictions vs True Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOxZaO87QbcU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Plot the actual training data\n",
        "plt.plot(df.index[:len(y_train)], y_train, label='Train Data')\n",
        "# Plot the actual test data\n",
        "plt.plot(df.index[len(y_train):len(y_train)+len(y_test)], y_test, label='Test Data')\n",
        "# Plot the model's predictions for the test data\n",
        "plt.plot(df.index[len(train_predictions):len(train_predictions)+len(test_predictions)],\n",
        "         test_predictions, label='Test Predictions')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNHkugYaCm9K"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "In the lab, we implemented some simple RNNs.  In this task implement a MultiStepRNNModel class that builds a recurrent neural network with the following architecture:\n",
        "\n",
        "- An RNN layer with the input size set to 1, hidden_size=20, and batch_first=True.\n",
        "\n",
        "- A second RNN layer with the input size set to 1, hidden_size=20, and batch_first=True.\n",
        "\n",
        "- A BatchNorm1d layer applied to the output of the final RNN layer at the last time step (the output should have 20 features).\n",
        "\n",
        "- A Dropout layer with the dropout ratio set by the dropout_prob parameter.\n",
        "\n",
        "- A final linear output layer that maps 20 features to 10 outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOIAtJWLCm9K"
      },
      "outputs": [],
      "source": [
        "class MultiStepRNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=20, output_size=10, dropout_prob=0.3):\n",
        "        # continue\n",
        "\n",
        "    def forward(self, x):\n",
        "        # continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeJMwqmSCm9K"
      },
      "source": [
        "---\n",
        "\n",
        "*End of Task 1. Copy your final code to **Homework 4 - Code** on **NextIlearn***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2h4UiY5Cm9K"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Previously, you have learned to build CNNs and MLPs from scratch.\n",
        "Now, you will apply your knowledge to create a model and fine-tune it on a real dataset!\n",
        "\n",
        "You have already recieved a folder `data` with this notebook. In this directory, you can find a `./data/Task2` directory. The `train` directory includes 5400 images, you can also find a file called `train_labels.csv` which includes the labels of you image train dataset. You can also find the `test` folder including 600 images as your test set.\n",
        "\n",
        "In this task, you will:\n",
        "\n",
        "Train CNN model on the provided dataset (either from scratch or using a pretrained model) on the training set provided.\n",
        "\n",
        "Predict the labels for test set of images provided in `test`.\n",
        "\n",
        "Save your predictions to a file `submission.csv`. Ensure your file is named `submission.csv` and includes a column titled `predictions`.\n",
        "\n",
        "You can design any CNN model from scratch or use any pretrained model (e.g., ResNet, VGG, DenseNet) from ```torchvision.models```.\n",
        "\n",
        "Your model must achieve at least 65% test accuracy on the test predictions to pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# load training data:\n",
        "data_train   = np.load('train_data.npy')\n",
        "labels_train = np.load('train_labels.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# labels:\n",
        "np.unique(labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data:\n",
        "import matplotlib.pyplot as plt\n",
        "i = np.random.randint(0, len(data_train))\n",
        "plt.imshow(data_train[i].reshape(3, 32, 32).transpose(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do something here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test = np.load('test_data.npy')\n",
        "predictions = predict(model, loader_test)\n",
        "\n",
        "pd.DataFrame(predictions, columns=['prediction']).to_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAnPn1r1Cm9K"
      },
      "source": [
        "---\n",
        "\n",
        "*End of Task 2. Upload your final predictions (the file* `submission.csv` *) to **Homework 4 - Code** on **NextIlearn***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
