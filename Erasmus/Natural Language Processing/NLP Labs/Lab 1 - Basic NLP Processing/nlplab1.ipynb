{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tested-office",
   "metadata": {},
   "source": [
    "## This is laboration exercise 1 Basic NLP processing in the Natural Language Processing - NLP course by Hercules Dalianis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-article",
   "metadata": {},
   "source": [
    "Download the BBB Politics and Business corpus \n",
    "http://people.dsv.su.se/hercules/BBC_politics_business.txt\n",
    "and put in a folder called NLP-lab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "antique-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Jupiter notebook or your Python interpreted and open the\n",
    "# Corpus file\n",
    "with open(\"BBC_politics_business.txt\") as f:\n",
    "    corpusText = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surface-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ippok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK library\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "harmful-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Corpus using the nltk.word_tokenize()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(corpusText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fixed-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84337 tokens\n"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "# Calculate the number of tokens in the corpus.\n",
    "# Think about it as length of a list.\n",
    "print(str(len(tokens)) + \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cosmetic-blocking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9763 types\n"
     ]
    }
   ],
   "source": [
    "# 2)\n",
    "# Calculate the number of unique tokens (types) there are in the corpus.\n",
    "# \"This is a token. This is another token.\" There are two occurancies of \"token\" in the previous\n",
    "# string but one type \"token\".\n",
    "# To calculate the types think about the set() function\n",
    "types = set(tokens)\n",
    "print(str(len(types)) + \" types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entertaining-concord",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 stop words\n"
     ]
    }
   ],
   "source": [
    "# Import the predefined stopwords in NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 3)\n",
    "# How many stopword are there in the stop word list?\n",
    "print(str(len(stop_words)) + \" stop words\")\n",
    "\n",
    "# Are this few or many stop words, can you think about\n",
    "# other stopwords?\n",
    "\n",
    "# There are 198 stopwords in the NLTK list. This is a decent amount, \n",
    "# but depending on the task, more stopwords might be needed.\n",
    "# For example, words like \"also\", \"could\", or domain-specific terms like \"news\" \n",
    "# might also be considered stopwords in some contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "known-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function that filters out the stopwords from\n",
    "# the BBC corpus\n",
    "def filter_stopwords(tokens):\n",
    "    filtered_tokens = [tokens for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "matched-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56299 tokens\n"
     ]
    }
   ],
   "source": [
    "# 4) \n",
    "# How many tokens are there in the BBC corpus after filter out the stopwords\n",
    "# are these reasonable or to many or too few?\n",
    "filtered_tokens = filter_stopwords(tokens)\n",
    "print(str(len(filtered_tokens)) + \" tokens\")\n",
    "\n",
    "# After removing stopwords, the number of tokens reduced from 84,337 to 56,299.\n",
    "# This is a reasonable reduction, since stopwords are very common in English.\n",
    "# The remaining tokens likely include the more meaningful words for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuing-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all stopwords really were removed from the BBC corpus\n",
    "# Can you write a Python function that also matches initial uppercase stopwords\n",
    "def filter_stopwords_lowercase(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Previously, stopwords with initial capital letters (e.g., \"The\", \"And\") were not removed\n",
    "# because the comparison was case-sensitive. By converting tokens to lowercase before \n",
    "# checking against the stopword list, we ensure all stopwords are properly filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conscious-apollo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54006 tokens\n"
     ]
    }
   ],
   "source": [
    "# 5)\n",
    "# How many tokens are there in the BBC corpus after filtering also\n",
    "# in lower case?\n",
    "filtered_tokens_lowercase = filter_stopwords_lowercase(tokens)\n",
    "print(str(len(filtered_tokens_lowercase)) + \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "square-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9565 types\n"
     ]
    }
   ],
   "source": [
    "# 6)\n",
    "# How many types do you have now?\n",
    "print(str(len(set(filtered_tokens_lowercase))) + \" types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sonic-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the WordNet lemmatizer in NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mobile-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function that lemmatizes the tokens in the BBC corpus\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "selective-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8815 types\n"
     ]
    }
   ],
   "source": [
    "# 7)\n",
    "# How many types do you have in the BBC corpus after lemmatization?\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens_lowercase)\n",
    "print(str(len(set(lemmatized_tokens))) + \" types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aquatic-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import PorterStemmer\n",
    "#stemmer =  PorterStemmer()\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "republican-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python function that stems the tokens in the BBC corpus\n",
    "def stem_tokens(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "specialized-greece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6506 types\n"
     ]
    }
   ],
   "source": [
    "# 8)\n",
    "# How many types do you have in the BBC Corpus after stemming?\n",
    "stemmed_tokens = stem_tokens(filtered_tokens_lowercase)\n",
    "print(str(len(set(stemmed_tokens))) + \" types\")\n",
    "\n",
    "# Are there any differences? If so, why?\n",
    "\n",
    "# After stemming, we have 6,506 types in the BBC corpus, while lemmatization gives 8,815 types.\n",
    "# The difference exists because stemming is more aggressiveâ€”it chops off word endings to reduce words\n",
    "# to a crude root form (e.g., \"running\" -> \"run\", \"national\" -> \"nation\") without considering grammar.\n",
    "# Lemmatization, in contrast, uses vocabulary and grammar rules to convert words to their proper base forms.\n",
    "# As a result, stemming yields fewer, but often less accurate, types compared to lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-spokesman",
   "metadata": {},
   "source": [
    "### Now we are ready with the corpus linguistic part of the laboration exercise and we start with POS taggning and Chunking which is more NLP processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confused-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nltk.pos_tag() function to POS tag the BBC corpus\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-duncan",
   "metadata": {},
   "source": [
    "9)\n",
    "Inspect the tagged corpus, what types of tags do you see?\n",
    "What do they mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-excess",
   "metadata": {},
   "source": [
    "Here follows an example with a short sentence to introduce chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "noted-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Israeli/JJ)\n",
      "(PERSON Ehud/NNP Olmert/NNP)\n",
      "(GPE Palestinian/NNP)\n",
      "(PERSON Mahmoud/NNP Abbas/NNP)\n",
      "(GPE Jerusalem/NNP)\n",
      "(GPE Palestinian/JJ)\n",
      "(PERSON Saeb/NNP Erekat/NNP)\n",
      "(GPE Israeli/JJ)\n",
      "(GPE Palestinian/JJ)\n",
      "(PERSON Mr./NNP Olmert/NNP)\n",
      "(PERSON Mr./NNP Olmert/NNP)\n",
      "(ORGANIZATION Kadima/NNP Party/NNP)\n"
     ]
    }
   ],
   "source": [
    "sent = (\"Israeli Prime Minister Ehud Olmert is scheduled to meet with Palestinian President Mahmoud Abbas Wednesday in Jerusalem. Palestinian negotiator Saeb Erekat said the talks will focus on permanent- status issues, Israeli checkpoints and the fate of Palestinian prisoners. The meeting, at Mr. Olmert's official residence, will be the first between the two since Mr. Olmert announced that he will step down after his Kadima Party chooses a new leader in September. The two leaders re-started peace talks in November with the goal of reaching a deal by this year's end\")\n",
    "\n",
    "# Tokenisation\n",
    "tokenised = nltk.word_tokenize(sent)\n",
    "\n",
    "# Taggning\n",
    "tagged = nltk.pos_tag(tokenised)\n",
    "\n",
    "# Chunking\n",
    "sentChunked = nltk.ne_chunk(tagged)\n",
    "\n",
    "#PERSON, ORGANIZATION and GPE\n",
    "\n",
    "#Print some named entities in the sentence\n",
    "for n in sentChunked:\n",
    "    if isinstance(n, nltk.tree.Tree):               \n",
    "        if n.label() == 'PERSON' or n.label() == 'ORGANIZATION' or n.label() == 'GPE':\n",
    "            print(n)\n",
    "        else:\n",
    "            n\n",
    "            #print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-arcade",
   "metadata": {},
   "source": [
    "Use the function draw() to draw a chunktree (shallow parse tree),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentChunked.draw()\n",
    "\n",
    "# If it doesn't work, try the lines below.\n",
    "# from IPython.display import SVG, display\n",
    "# display(SVG(sentChunked.__repr_svg_()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-intermediate",
   "metadata": {},
   "source": [
    "10)\n",
    "Count the number of Persons, Organisation and GPEs, Geo Political Entities in the BBC corpus. Follow the above example and extend the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-replication",
   "metadata": {},
   "source": [
    "The chunking of the BBC corpus takes some time, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "devoted-runner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking\n"
     ]
    }
   ],
   "source": [
    "print('Chunking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "expanded-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging\n",
    "tagged = nltk.pos_tag(lemmatized_tokens)\n",
    "\n",
    "# Chunking\n",
    "sentChunked = nltk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complex-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PERSON: 2360\n"
     ]
    }
   ],
   "source": [
    "# Number of PERSON\n",
    "PERSON_NUM = 0\n",
    "\n",
    "for n in sentChunked:\n",
    "    if isinstance(n, nltk.tree.Tree):               \n",
    "        if n.label() == 'PERSON':\n",
    "            PERSON_NUM += 1\n",
    "\n",
    "print(\"Number of PERSON: \" + str(PERSON_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "photographic-settle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ORGANIZATION: 989\n"
     ]
    }
   ],
   "source": [
    "# Number of ORGANIZATION\n",
    "ORGANIZATION_NUM = 0\n",
    "\n",
    "for n in sentChunked:\n",
    "    if isinstance(n, nltk.tree.Tree):               \n",
    "        if n.label() == 'ORGANIZATION':\n",
    "            ORGANIZATION_NUM += 1\n",
    "\n",
    "print(\"Number of ORGANIZATION: \" + str(ORGANIZATION_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "norwegian-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPE: 1010\n"
     ]
    }
   ],
   "source": [
    "# Number of GPE/locations\n",
    "GPE_NUM = 0\n",
    "\n",
    "for n in sentChunked:\n",
    "    if isinstance(n, nltk.tree.Tree):               \n",
    "        if n.label() == 'GPE':\n",
    "            GPE_NUM += 1\n",
    "\n",
    "print(\"Number of GPE: \" + str(GPE_NUM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-tours",
   "metadata": {},
   "source": [
    "### Now we will study N-grams\n",
    "See example below, fill in the missing code.\n",
    "\n",
    "11) Are there any bigrams or trigram or ngrams that give\n",
    "logical groups of texts/grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "continuing-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "sent = (\"Israeli Prime Minister Ehud Olmert is scheduled to meet with Palestinian President Mahmoud Abbas Wednesday in Jerusalem. Palestinian negotiator Saeb Erekat said the talks will focus on permanent- status issues, Israeli checkpoints and the fate of Palestinian prisoners. The meeting, at Mr. Olmert's official residence, will be the first between the two since Mr. Olmert announced that he will step down after his Kadima Party chooses a new leader in September. The two leaders re-started peace talks in November with the goal of reaching a deal by this year's end\")\n",
    "tokenised = nltk.word_tokenize(sent)\n",
    "\n",
    "bigrams_list = bigrams(tokenised)\n",
    "trigrams_list = trigrams(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0f003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Israeli', 'Prime')\n",
      "('Prime', 'Minister')\n",
      "('Minister', 'Ehud')\n",
      "('Ehud', 'Olmert')\n",
      "('Olmert', 'is')\n",
      "('is', 'scheduled')\n",
      "('scheduled', 'to')\n",
      "('to', 'meet')\n",
      "('meet', 'with')\n",
      "('with', 'Palestinian')\n",
      "('Palestinian', 'President')\n",
      "('President', 'Mahmoud')\n",
      "('Mahmoud', 'Abbas')\n",
      "('Abbas', 'Wednesday')\n",
      "('Wednesday', 'in')\n",
      "('in', 'Jerusalem')\n",
      "('Jerusalem', '.')\n",
      "('.', 'Palestinian')\n",
      "('Palestinian', 'negotiator')\n",
      "('negotiator', 'Saeb')\n",
      "('Saeb', 'Erekat')\n",
      "('Erekat', 'said')\n",
      "('said', 'the')\n",
      "('the', 'talks')\n",
      "('talks', 'will')\n",
      "('will', 'focus')\n",
      "('focus', 'on')\n",
      "('on', 'permanent-')\n",
      "('permanent-', 'status')\n",
      "('status', 'issues')\n",
      "('issues', ',')\n",
      "(',', 'Israeli')\n",
      "('Israeli', 'checkpoints')\n",
      "('checkpoints', 'and')\n",
      "('and', 'the')\n",
      "('the', 'fate')\n",
      "('fate', 'of')\n",
      "('of', 'Palestinian')\n",
      "('Palestinian', 'prisoners')\n",
      "('prisoners', '.')\n",
      "('.', 'The')\n",
      "('The', 'meeting')\n",
      "('meeting', ',')\n",
      "(',', 'at')\n",
      "('at', 'Mr.')\n",
      "('Mr.', 'Olmert')\n",
      "('Olmert', \"'s\")\n",
      "(\"'s\", 'official')\n",
      "('official', 'residence')\n",
      "('residence', ',')\n",
      "(',', 'will')\n",
      "('will', 'be')\n",
      "('be', 'the')\n",
      "('the', 'first')\n",
      "('first', 'between')\n",
      "('between', 'the')\n",
      "('the', 'two')\n",
      "('two', 'since')\n",
      "('since', 'Mr.')\n",
      "('Mr.', 'Olmert')\n",
      "('Olmert', 'announced')\n",
      "('announced', 'that')\n",
      "('that', 'he')\n",
      "('he', 'will')\n",
      "('will', 'step')\n",
      "('step', 'down')\n",
      "('down', 'after')\n",
      "('after', 'his')\n",
      "('his', 'Kadima')\n",
      "('Kadima', 'Party')\n",
      "('Party', 'chooses')\n",
      "('chooses', 'a')\n",
      "('a', 'new')\n",
      "('new', 'leader')\n",
      "('leader', 'in')\n",
      "('in', 'September')\n",
      "('September', '.')\n",
      "('.', 'The')\n",
      "('The', 'two')\n",
      "('two', 'leaders')\n",
      "('leaders', 're-started')\n",
      "('re-started', 'peace')\n",
      "('peace', 'talks')\n",
      "('talks', 'in')\n",
      "('in', 'November')\n",
      "('November', 'with')\n",
      "('with', 'the')\n",
      "('the', 'goal')\n",
      "('goal', 'of')\n",
      "('of', 'reaching')\n",
      "('reaching', 'a')\n",
      "('a', 'deal')\n",
      "('deal', 'by')\n",
      "('by', 'this')\n",
      "('this', 'year')\n",
      "('year', \"'s\")\n",
      "(\"'s\", 'end')\n"
     ]
    }
   ],
   "source": [
    "for bigram in bigrams_list:\n",
    "  print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23e4ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Israeli', 'Prime', 'Minister')\n",
      "('Prime', 'Minister', 'Ehud')\n",
      "('Minister', 'Ehud', 'Olmert')\n",
      "('Ehud', 'Olmert', 'is')\n",
      "('Olmert', 'is', 'scheduled')\n",
      "('is', 'scheduled', 'to')\n",
      "('scheduled', 'to', 'meet')\n",
      "('to', 'meet', 'with')\n",
      "('meet', 'with', 'Palestinian')\n",
      "('with', 'Palestinian', 'President')\n",
      "('Palestinian', 'President', 'Mahmoud')\n",
      "('President', 'Mahmoud', 'Abbas')\n",
      "('Mahmoud', 'Abbas', 'Wednesday')\n",
      "('Abbas', 'Wednesday', 'in')\n",
      "('Wednesday', 'in', 'Jerusalem')\n",
      "('in', 'Jerusalem', '.')\n",
      "('Jerusalem', '.', 'Palestinian')\n",
      "('.', 'Palestinian', 'negotiator')\n",
      "('Palestinian', 'negotiator', 'Saeb')\n",
      "('negotiator', 'Saeb', 'Erekat')\n",
      "('Saeb', 'Erekat', 'said')\n",
      "('Erekat', 'said', 'the')\n",
      "('said', 'the', 'talks')\n",
      "('the', 'talks', 'will')\n",
      "('talks', 'will', 'focus')\n",
      "('will', 'focus', 'on')\n",
      "('focus', 'on', 'permanent-')\n",
      "('on', 'permanent-', 'status')\n",
      "('permanent-', 'status', 'issues')\n",
      "('status', 'issues', ',')\n",
      "('issues', ',', 'Israeli')\n",
      "(',', 'Israeli', 'checkpoints')\n",
      "('Israeli', 'checkpoints', 'and')\n",
      "('checkpoints', 'and', 'the')\n",
      "('and', 'the', 'fate')\n",
      "('the', 'fate', 'of')\n",
      "('fate', 'of', 'Palestinian')\n",
      "('of', 'Palestinian', 'prisoners')\n",
      "('Palestinian', 'prisoners', '.')\n",
      "('prisoners', '.', 'The')\n",
      "('.', 'The', 'meeting')\n",
      "('The', 'meeting', ',')\n",
      "('meeting', ',', 'at')\n",
      "(',', 'at', 'Mr.')\n",
      "('at', 'Mr.', 'Olmert')\n",
      "('Mr.', 'Olmert', \"'s\")\n",
      "('Olmert', \"'s\", 'official')\n",
      "(\"'s\", 'official', 'residence')\n",
      "('official', 'residence', ',')\n",
      "('residence', ',', 'will')\n",
      "(',', 'will', 'be')\n",
      "('will', 'be', 'the')\n",
      "('be', 'the', 'first')\n",
      "('the', 'first', 'between')\n",
      "('first', 'between', 'the')\n",
      "('between', 'the', 'two')\n",
      "('the', 'two', 'since')\n",
      "('two', 'since', 'Mr.')\n",
      "('since', 'Mr.', 'Olmert')\n",
      "('Mr.', 'Olmert', 'announced')\n",
      "('Olmert', 'announced', 'that')\n",
      "('announced', 'that', 'he')\n",
      "('that', 'he', 'will')\n",
      "('he', 'will', 'step')\n",
      "('will', 'step', 'down')\n",
      "('step', 'down', 'after')\n",
      "('down', 'after', 'his')\n",
      "('after', 'his', 'Kadima')\n",
      "('his', 'Kadima', 'Party')\n",
      "('Kadima', 'Party', 'chooses')\n",
      "('Party', 'chooses', 'a')\n",
      "('chooses', 'a', 'new')\n",
      "('a', 'new', 'leader')\n",
      "('new', 'leader', 'in')\n",
      "('leader', 'in', 'September')\n",
      "('in', 'September', '.')\n",
      "('September', '.', 'The')\n",
      "('.', 'The', 'two')\n",
      "('The', 'two', 'leaders')\n",
      "('two', 'leaders', 're-started')\n",
      "('leaders', 're-started', 'peace')\n",
      "('re-started', 'peace', 'talks')\n",
      "('peace', 'talks', 'in')\n",
      "('talks', 'in', 'November')\n",
      "('in', 'November', 'with')\n",
      "('November', 'with', 'the')\n",
      "('with', 'the', 'goal')\n",
      "('the', 'goal', 'of')\n",
      "('goal', 'of', 'reaching')\n",
      "('of', 'reaching', 'a')\n",
      "('reaching', 'a', 'deal')\n",
      "('a', 'deal', 'by')\n",
      "('deal', 'by', 'this')\n",
      "('by', 'this', 'year')\n",
      "('this', 'year', \"'s\")\n",
      "('year', \"'s\", 'end')\n"
     ]
    }
   ],
   "source": [
    "for trigram in trigrams_list:\n",
    "  print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c49841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, some bigrams and trigrams form logical groupings.\n",
    "# For example, bigrams like (\"Prime\", \"Minister\") and (\"peace\", \"talks\"),\n",
    "# or trigrams like (\"Prime\", \"Minister\", \"Ehud\") and (\"Palestinian\", \"President\", \"Mahmoud\")\n",
    "# represent meaningful phrases in the context of the sentence.\n",
    "# These n-grams capture named entities and common multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-fault",
   "metadata": {},
   "source": [
    "### Now we will do some regular expression matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "inside-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden',\n",
       " 'Sweden']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import regular expression library for Python\n",
    "import re\n",
    "\n",
    "# Match all Sweden in the corpusText\n",
    "re.findall(r\"Sweden\", corpusText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-relief",
   "metadata": {},
   "source": [
    "Do the same for Sweden again but extract also the snippet/text around them. \n",
    "\n",
    "12) What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "neither-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the UK third behind Sweden and Ireland for bus',\n",
       " 'ly, Switzerland and Sweden will be stored perm',\n",
       " 'l sides of Ireland, Sweden and Norway. Shares ',\n",
       " 'end up producing in Sweden?\"',\n",
       " ' build Cadillacs in Sweden',\n",
       " 'ing Saab factory in Sweden.',\n",
       " 'lloyed good news in Sweden, since it reflects ',\n",
       " ' American marque in Sweden is part of its effo',\n",
       " 'illac production to Sweden should help introdu']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\".{0,20}Sweden.{0,20}\", corpusText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a33fe3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The context snippets around \"Sweden\" show it's mostly mentioned in political, economic, or industrial settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-lyric",
   "metadata": {},
   "source": [
    "Do the same for the car company SAAB but extract also the snippet/text around them. \n",
    "\n",
    "13) What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "academic-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pean units Opel and Saab have both had troub',\n",
       " 'Saab to build Cadillacs ',\n",
       " ' at its loss-making Saab factory in Sweden.',\n",
       " \" allay fears of the Saab factory's closure. \",\n",
       " \", since it reflects Saab's failure to make s\",\n",
       " ' market. For years, Saab has consistently sa',\n",
       " 'needed scale to the Saab factory, which curr',\n",
       " 'ble operations, and Saab is losing money fas',\n",
       " 'nditure by building Saabs, Opels - badged as',\n",
       " \"y to further reduce Saab's losses could be t\",\n",
       " 'f the production of Saabs to the US, a marke',\n",
       " 'ar, which is making Saabs more expensive to ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\".{0,20}Saab.{0,20}\", corpusText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d5af1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The snippets around \"Saab\" show that the company is mostly discussed in the context of automotive manufacturing,\n",
    "# business challenges, and economic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-samba",
   "metadata": {},
   "source": [
    "Combine the two regular expressions so get both Sweden and Saab in the same context.\n",
    "\n",
    "14) Are the results comprehensible? How can you improve them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worthy-collar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saab to build Cadillacs in Sweden',\n",
       " ' at its loss-making Saab factory in Sweden.',\n",
       " \"lloyed good news in Sweden, since it reflects Saab's failure to make s\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"(?:.{0,20}Sweden.{0,20}.{0,20}Saab.{0,20}|.{0,20}Saab.{0,20}.{0,20}Sweden.{0,20})\", corpusText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are mostly clear but could be improved:\n",
    "# 1) Some matches are cut off - fix by using word boundaries (\\bSweden\\b)\n",
    "# 2) Context is limited - expand window size (.{0,50}) or match full sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-dynamics",
   "metadata": {},
   "source": [
    "### Here is an extra task which is not compulsary.\n",
    "Match all organisations that is close to Sweden in the BBC corpus and print the snippets.\n",
    "\n",
    "15) What do you get? Are there repetitions? How can one avoid them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-scientist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing Cadillac production to Sweden should help introduce desperately-needed scale to the Saab factory, which currently produces fewer than 130,000 cars per year.\n",
      "Intermediate waste from countries such as Japan, Germany, Spain, Italy, Switzerland and Sweden will be stored permanently in the UK.\n",
      "Saab to build Cadillacs in Sweden\n",
      "\n",
      "General Motors, the world's largest car maker, has confirmed that it will build a new medium-sized Cadillac BLS at its loss-making Saab factory in Sweden.\n",
      "The report, carried out independently by consultants Booz Allen Hamilton and HI Europe, placed the UK third behind Sweden and Ireland for business use of ICT.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import ne_chunk\n",
    "\n",
    "sentences = sent_tokenize(corpusText)\n",
    "\n",
    "snippets = set()\n",
    "\n",
    "for sentence in sentences:\n",
    "    if \"Sweden\" in sentence:\n",
    "        words = word_tokenize(sentence)\n",
    "        tagged = pos_tag(words)\n",
    "        chunked = ne_chunk(tagged)\n",
    "\n",
    "        for subtree in chunked:\n",
    "            if hasattr(subtree, 'label') and subtree.label() == 'ORGANIZATION':\n",
    "                snippets.add(sentence.strip())\n",
    "\n",
    "for snippet in snippets:\n",
    "    print(snippet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
